{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import gymnasium.envs.box2d as box2d\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "box2d.car_racing.STATE_W, box2d.car_racing.STATE_H = 64, 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (64, 64, 3), uint8)\n",
      "Action space:  Box([-1.  0.  0.], 1.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2')\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, info = env.reset()\n",
    "print(s.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(s)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    # img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the original environment.\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "        \n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        # s = preprocess(s)\n",
    "\n",
    "        # The initial observation is simply a copy of the frame `s`\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # We take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        # s = preprocess(s)\n",
    "\n",
    "        # Push the current frame `s` at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (64, 64, 3)\n",
      "Shape of \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "print(\"The shape of an observation: \", s.shape)\n",
    "print(\"Shape of \")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(s)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (64, 64, 3)\n",
      "Shape of \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs, reward, done, a, b = env.step([0,1,0])\n",
    "print(\"The shape of an observation: \", obs.shape)\n",
    "print(\"Shape of \")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(obs)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Variational encoder model, used as a visual model\n",
    "for our model of the world.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" VAE decoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, 1024)\n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 6, stride=2)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        reconstruction = F.sigmoid(self.deconv4(x))\n",
    "        return reconstruction\n",
    "\n",
    "class Encoder(nn.Module): # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" VAE encoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        #self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(img_channels, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n",
    "\n",
    "        self.fc_mu = nn.Linear(2*2*256, latent_size)\n",
    "        self.fc_logsigma = nn.Linear(2*2*256, latent_size)\n",
    "\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logsigma = self.fc_logsigma(x)\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\" Variational Autoencoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, latent_size)\n",
    "        self.decoder = Decoder(img_channels, latent_size)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        mu, logsigma = self.encoder(x)\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logsigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define MDRNN model, supposed to be used as a world model\n",
    "on the latent space.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "def gmm_loss(batch, mus, sigmas, logpi, reduce=True): # pylint: disable=too-many-arguments\n",
    "    \"\"\" Computes the gmm loss.\n",
    "\n",
    "    Compute minus the log probability of batch under the GMM model described\n",
    "    by mus, sigmas, pi. Precisely, with bs1, bs2, ... the sizes of the batch\n",
    "    dimensions (several batch dimension are useful when you have both a batch\n",
    "    axis and a time step axis), gs the number of mixtures and fs the number of\n",
    "    features.\n",
    "\n",
    "    :args batch: (bs1, bs2, *, fs) torch tensor\n",
    "    :args mus: (bs1, bs2, *, gs, fs) torch tensor\n",
    "    :args sigmas: (bs1, bs2, *, gs, fs) torch tensor\n",
    "    :args logpi: (bs1, bs2, *, gs) torch tensor\n",
    "    :args reduce: if not reduce, the mean in the following formula is ommited\n",
    "\n",
    "    :returns:\n",
    "    loss(batch) = - mean_{i1=0..bs1, i2=0..bs2, ...} log(\n",
    "        sum_{k=1..gs} pi[i1, i2, ..., k] * N(\n",
    "            batch[i1, i2, ..., :] | mus[i1, i2, ..., k, :], sigmas[i1, i2, ..., k, :]))\n",
    "\n",
    "    NOTE: The loss is not reduced along the feature dimension (i.e. it should scale ~linearily\n",
    "    with fs).\n",
    "    \"\"\"\n",
    "    batch = batch.unsqueeze(-2)\n",
    "    normal_dist = Normal(mus, sigmas)\n",
    "    g_log_probs = normal_dist.log_prob(batch)\n",
    "    g_log_probs = logpi + torch.sum(g_log_probs, dim=-1)\n",
    "    max_log_probs = torch.max(g_log_probs, dim=-1, keepdim=True)[0]\n",
    "    g_log_probs = g_log_probs - max_log_probs\n",
    "\n",
    "    g_probs = torch.exp(g_log_probs)\n",
    "    probs = torch.sum(g_probs, dim=-1)\n",
    "\n",
    "    log_prob = max_log_probs.squeeze() + torch.log(probs)\n",
    "    if reduce:\n",
    "        return - torch.mean(log_prob)\n",
    "    return - log_prob\n",
    "\n",
    "class _MDRNNBase(nn.Module):\n",
    "    def __init__(self, latents, actions, hiddens, gaussians):\n",
    "        super().__init__()\n",
    "        self.latents = latents\n",
    "        self.actions = actions\n",
    "        self.hiddens = hiddens\n",
    "        self.gaussians = gaussians\n",
    "\n",
    "        self.gmm_linear = nn.Linear(\n",
    "            hiddens, (2 * latents + 1) * gaussians + 2)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        pass\n",
    "\n",
    "class MDRNN(_MDRNNBase):\n",
    "    \"\"\" MDRNN model for multi steps forward \"\"\"\n",
    "    def __init__(self, latents, actions, hiddens, gaussians):\n",
    "        super().__init__(latents, actions, hiddens, gaussians)\n",
    "        self.rnn = nn.LSTM(latents + actions, hiddens)\n",
    "\n",
    "    def forward(self, actions, latents): # pylint: disable=arguments-differ\n",
    "        \"\"\" MULTI STEPS forward.\n",
    "\n",
    "        :args actions: (SEQ_LEN, BSIZE, ASIZE) torch tensor\n",
    "        :args latents: (SEQ_LEN, BSIZE, LSIZE) torch tensor\n",
    "\n",
    "        :returns: mu_nlat, sig_nlat, pi_nlat, rs, ds, parameters of the GMM\n",
    "        prediction for the next latent, gaussian prediction of the reward and\n",
    "        logit prediction of terminality.\n",
    "            - mu_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n",
    "            - sigma_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n",
    "            - logpi_nlat: (SEQ_LEN, BSIZE, N_GAUSS) torch tensor\n",
    "            - rs: (SEQ_LEN, BSIZE) torch tensor\n",
    "            - ds: (SEQ_LEN, BSIZE) torch tensor\n",
    "        \"\"\"\n",
    "        seq_len, bs = actions.size(0), actions.size(1)\n",
    "\n",
    "        ins = torch.cat([actions, latents], dim=-1)\n",
    "        outs, _ = self.rnn(ins)\n",
    "        gmm_outs = self.gmm_linear(outs)\n",
    "\n",
    "        stride = self.gaussians * self.latents\n",
    "\n",
    "        mus = gmm_outs[:, :, :stride]\n",
    "        mus = mus.view(seq_len, bs, self.gaussians, self.latents)\n",
    "\n",
    "        sigmas = gmm_outs[:, :, stride:2 * stride]\n",
    "        sigmas = sigmas.view(seq_len, bs, self.gaussians, self.latents)\n",
    "        sigmas = torch.exp(sigmas)\n",
    "\n",
    "        pi = gmm_outs[:, :, 2 * stride: 2 * stride + self.gaussians]\n",
    "        pi = pi.view(seq_len, bs, self.gaussians)\n",
    "        logpi = f.log_softmax(pi, dim=-1)\n",
    "\n",
    "        rs = gmm_outs[:, :, -2]\n",
    "\n",
    "        ds = gmm_outs[:, :, -1]\n",
    "\n",
    "        return mus, sigmas, logpi, rs, ds\n",
    "\n",
    "class MDRNNCell(_MDRNNBase):\n",
    "    \"\"\" MDRNN model for one step forward \"\"\"\n",
    "    def __init__(self, latents, actions, hiddens, gaussians):\n",
    "        super().__init__(latents, actions, hiddens, gaussians)\n",
    "        self.rnn = nn.LSTMCell(latents + actions, hiddens)\n",
    "\n",
    "    def forward(self, action, latent, hidden): # pylint: disable=arguments-differ\n",
    "        \"\"\" ONE STEP forward.\n",
    "\n",
    "        :args actions: (BSIZE, ASIZE) torch tensor\n",
    "        :args latents: (BSIZE, LSIZE) torch tensor\n",
    "        :args hidden: (BSIZE, RSIZE) torch tensor\n",
    "\n",
    "        :returns: mu_nlat, sig_nlat, pi_nlat, r, d, next_hidden, parameters of\n",
    "        the GMM prediction for the next latent, gaussian prediction of the\n",
    "        reward, logit prediction of terminality and next hidden state.\n",
    "            - mu_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n",
    "            - sigma_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n",
    "            - logpi_nlat: (BSIZE, N_GAUSS) torch tensor\n",
    "            - rs: (BSIZE) torch tensor\n",
    "            - ds: (BSIZE) torch tensor\n",
    "        \"\"\"\n",
    "        in_al = torch.cat([action, latent], dim=1)\n",
    "\n",
    "        next_hidden = self.rnn(in_al, hidden)\n",
    "        out_rnn = next_hidden[0]\n",
    "\n",
    "        out_full = self.gmm_linear(out_rnn)\n",
    "\n",
    "        stride = self.gaussians * self.latents\n",
    "\n",
    "        mus = out_full[:, :stride]\n",
    "        mus = mus.view(-1, self.gaussians, self.latents)\n",
    "\n",
    "        sigmas = out_full[:, stride:2 * stride]\n",
    "        sigmas = sigmas.view(-1, self.gaussians, self.latents)\n",
    "        sigmas = torch.exp(sigmas)\n",
    "\n",
    "        pi = out_full[:, 2 * stride:2 * stride + self.gaussians]\n",
    "        pi = pi.view(-1, self.gaussians)\n",
    "        logpi = f.log_softmax(pi, dim=-1)\n",
    "\n",
    "        r = out_full[:, -2]\n",
    "\n",
    "        d = out_full[:, -1]\n",
    "\n",
    "        return mus, sigmas, logpi, r, d, next_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define controller \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    \"\"\" Controller \"\"\"\n",
    "    def __init__(self, latents, recurrents, actions):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latents + recurrents, actions)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        cat_in = torch.cat(inputs, dim=1)\n",
    "        return self.fc(cat_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hardcoded for now\n",
    "ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE =\\\n",
    "    3, 32, 256, 64, 64\n",
    "\n",
    "# Same\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((RED_SIZE, RED_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def sample_continuous_policy(action_space, seq_len, dt):\n",
    "    \"\"\" Sample a continuous policy.\n",
    "\n",
    "    Atm, action_space is supposed to be a box environment. The policy is\n",
    "    sampled as a brownian motion a_{t+1} = a_t + sqrt(dt) N(0, 1).\n",
    "\n",
    "    :args action_space: gym action space\n",
    "    :args seq_len: number of actions returned\n",
    "    :args dt: temporal discretization\n",
    "\n",
    "    :returns: sequence of seq_len actions\n",
    "    \"\"\"\n",
    "    actions = [action_space.sample()]\n",
    "    for _ in range(seq_len):\n",
    "        daction_dt = np.random.randn(*actions[-1].shape)\n",
    "        actions.append(\n",
    "            np.clip(actions[-1] + math.sqrt(dt) * daction_dt,\n",
    "                    action_space.low, action_space.high))\n",
    "    return actions\n",
    "\n",
    "def save_checkpoint(state, is_best, filename, best_filename):\n",
    "    \"\"\" Save state in filename. Also save in best_filename if is_best. \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        torch.save(state, best_filename)\n",
    "\n",
    "def flatten_parameters(params):\n",
    "    \"\"\" Flattening parameters.\n",
    "\n",
    "    :args params: generator of parameters (as returned by module.parameters())\n",
    "\n",
    "    :returns: flattened parameters (i.e. one tensor of dimension 1 with all\n",
    "        parameters concatenated)\n",
    "    \"\"\"\n",
    "    return torch.cat([p.detach().view(-1) for p in params], dim=0).cpu().numpy()\n",
    "\n",
    "def unflatten_parameters(params, example, device):\n",
    "    \"\"\" Unflatten parameters.\n",
    "\n",
    "    :args params: parameters as a single 1D np array\n",
    "    :args example: generator of parameters (as returned by module.parameters()),\n",
    "        used to reshape params\n",
    "    :args device: where to store unflattened parameters\n",
    "\n",
    "    :returns: unflattened parameters\n",
    "    \"\"\"\n",
    "    params = torch.Tensor(params).to(device)\n",
    "    idx = 0\n",
    "    unflattened = []\n",
    "    for e_p in example:\n",
    "        unflattened += [params[idx:idx + e_p.numel()].view(e_p.size())]\n",
    "        idx += e_p.numel()\n",
    "    return unflattened\n",
    "\n",
    "def load_parameters(params, controller):\n",
    "    \"\"\" Load flattened parameters into controller.\n",
    "\n",
    "    :args params: parameters as a single 1D np array\n",
    "    :args controller: module in which params is loaded\n",
    "    \"\"\"\n",
    "    proto = next(controller.parameters())\n",
    "    params = unflatten_parameters(\n",
    "        params, controller.parameters(), proto.device)\n",
    "\n",
    "    for p, p_0 in zip(controller.parameters(), params):\n",
    "        p.data.copy_(p_0)\n",
    "        \n",
    "class RolloutGenerator(object):\n",
    "    \"\"\" Utility to generate rollouts.\n",
    "\n",
    "    Encapsulate everything that is needed to generate rollouts in the TRUE ENV\n",
    "    using a controller with previously trained VAE and MDRNN.\n",
    "\n",
    "    :attr vae: VAE model loaded from mdir/vae\n",
    "    :attr mdrnn: MDRNN model loaded from mdir/mdrnn\n",
    "    :attr controller: Controller, either loaded from mdir/ctrl or randomly\n",
    "        initialized\n",
    "    :attr env: instance of the CarRacing-v0 gym environment\n",
    "    :attr device: device used to run VAE, MDRNN and Controller\n",
    "    :attr time_limit: rollouts have a maximum of time_limit timesteps\n",
    "    \"\"\"\n",
    "    def __init__(self, mdir, device, time_limit):\n",
    "        \"\"\" Build vae, rnn, controller and environment. \"\"\"\n",
    "        # Loading world model and vae\n",
    "        vae_file, rnn_file, ctrl_file = \\\n",
    "            [join(mdir, m, 'best.tar') for m in ['vae', 'mdrnn', 'ctrl']]\n",
    "\n",
    "        assert exists(vae_file) and exists(rnn_file),\\\n",
    "            \"Either vae or mdrnn is untrained.\"\n",
    "\n",
    "        vae_state, rnn_state = [\n",
    "            torch.load(fname, map_location={'cuda:0': str(device)})\n",
    "            for fname in (vae_file, rnn_file)]\n",
    "\n",
    "        for m, s in (('VAE', vae_state), ('MDRNN', rnn_state)):\n",
    "            print(\"Loading {} at epoch {} \"\n",
    "                  \"with test loss {}\".format(\n",
    "                      m, s['epoch'], s['precision']))\n",
    "\n",
    "        self.vae = VAE(3, LSIZE).to(device)\n",
    "        self.vae.load_state_dict(vae_state['state_dict'])\n",
    "\n",
    "        self.mdrnn = MDRNNCell(LSIZE, ASIZE, RSIZE, 5).to(device)\n",
    "        self.mdrnn.load_state_dict(\n",
    "            {k.strip('_l0'): v for k, v in rnn_state['state_dict'].items()})\n",
    "\n",
    "        self.controller = Controller(LSIZE, RSIZE, ASIZE).to(device)\n",
    "\n",
    "        # load controller if it was previously saved\n",
    "        if exists(ctrl_file):\n",
    "            ctrl_state = torch.load(ctrl_file, map_location={'cuda:0': str(device)})\n",
    "            print(\"Loading Controller with reward {}\".format(\n",
    "                ctrl_state['reward']))\n",
    "            self.controller.load_state_dict(ctrl_state['state_dict'])\n",
    "\n",
    "        self.env = gym.make('CarRacing-v2')\n",
    "        self.device = device\n",
    "\n",
    "        self.time_limit = time_limit\n",
    "\n",
    "    def get_action_and_transition(self, obs, hidden):\n",
    "        \"\"\" Get action and transition.\n",
    "\n",
    "        Encode obs to latent using the VAE, then obtain estimation for next\n",
    "        latent and next hidden state using the MDRNN and compute the controller\n",
    "        corresponding action.\n",
    "\n",
    "        :args obs: current observation (1 x 3 x 64 x 64) torch tensor\n",
    "        :args hidden: current hidden state (1 x 256) torch tensor\n",
    "\n",
    "        :returns: (action, next_hidden)\n",
    "            - action: 1D np array\n",
    "            - next_hidden (1 x 256) torch tensor\n",
    "        \"\"\"\n",
    "        print(\"\")\n",
    "        _, latent_mu, _ = self.vae(obs)\n",
    "        action = self.controller(latent_mu, hidden[0])\n",
    "        _, _, _, _, _, next_hidden = self.mdrnn(action, latent_mu, hidden)\n",
    "        return action.squeeze().cpu().detach().numpy(), next_hidden\n",
    "\n",
    "    def rollout(self, params, render=False):\n",
    "        \"\"\" Execute a rollout and returns minus cumulative reward.\n",
    "\n",
    "        Load :params: into the controller and execute a single rollout. This\n",
    "        is the main API of this class.\n",
    "\n",
    "        :args params: parameters as a single 1D np array\n",
    "\n",
    "        :returns: minus cumulative reward\n",
    "        \"\"\"\n",
    "        # copy params into the controller\n",
    "        if params is not None:\n",
    "            load_parameters(params, self.controller)\n",
    "\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        # This first render is required !\n",
    "        self.env.render()\n",
    "\n",
    "        hidden = [\n",
    "            torch.zeros(1, RSIZE).to(self.device)\n",
    "            for _ in range(2)]\n",
    "\n",
    "        cumulative = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            obs = transform(obs).unsqueeze(0).to(self.device)\n",
    "            action, hidden = self.get_action_and_transition(obs, hidden)\n",
    "            obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            cumulative += reward\n",
    "            if done or i > self.time_limit:\n",
    "                return - cumulative\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VAE at epoch 76 with test loss 22.783294927978517\n",
      "Loading MDRNN at epoch 91 with test loss 1.0236724321507225\n"
     ]
    }
   ],
   "source": [
    "r_gen = RolloutGenerator(\"exp_dir\", device=\"cpu\",time_limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos transf  torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "obs = r_gen.env.reset()\n",
    "# print(\"This is \",obs)\n",
    "# print(\"This is obs shape\",obs)\n",
    "# This first render is required !\n",
    "r_gen.env.render()\n",
    "\n",
    "hidden = [\n",
    "    torch.zeros(1, RSIZE).to(r_gen.device)\n",
    "    for _ in range(2)]\n",
    "\n",
    "cumulative = 0\n",
    "i = 0\n",
    "obs = transform(obs[0]).unsqueeze(0).to(r_gen.device)\n",
    "# while (i<5):\n",
    "print(\"Pos transf \",np.shape(obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  torch.Size([1, 3, 64, 64])\n",
      "\n",
      "next obs  (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"obs \",np.shape(obs))\n",
    "action, hidden = r_gen.get_action_and_transition(obs, hidden)\n",
    "obs, reward, done, a, b = r_gen.env.step(action)\n",
    "# obs = transform(obs[0]).unsqueeze(0).to(r_gen.device)\n",
    "print(\"next obs \",np.shape(obs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
